{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "The project work implements the paper Students need more attention which mainly discusses on modifying Bi-directional Encoder Representations from Transformers(BERT) model to classify text data which are imbalanced and small. The general observation from authors is even though deep learning models like CNN, LSTM, BERT perform better than traditional models like SVM, linear regression they tend to overfit for use cases where the dataset is small and imbalanced.\n",
        "\n",
        "**Problem:**\n",
        "\n",
        "Deep learning models, like BERT, are powerful tools but require a vast amount of data for training.\n",
        "In healthcare, datasets are often limited due to privacy concerns and the nature of medical information.\n",
        "Small datasets can lead to overfitting, where the model performs well on the training data but poorly on unseen data.\n",
        "\n",
        "**Proposed Solution:**\n",
        "\n",
        "The paper introduces a novel framework called LESA-BERT (Label Embeddings for Self-Attention BERT). It builds upon BioBERT, a pre-trained model specifically for biomedical text mining. LESA-BERT incorporates two key elements:\n",
        "\n",
        "**1. Label Embeddings for Self-Attention:** This injects label information into the self-attention mechanism of BERT at every layer. Self-attention allows the model to focus on important parts of the text relevant to the task. By including label information, the model can better learn patterns that differentiate between urgency levels in patient messages.\n",
        "\n",
        "**2. Knowledge Distillation:** Large models like LESA-BERT can be prone to overfitting on small datasets. The paper addresses this by employing a technique called knowledge distillation. Here, a smaller \"student\" model is trained to mimic the predictions of the larger \"teacher\" model (BERT). This compressed student model inherits the knowledge of the teacher but with fewer parameters, reducing overfitting risks.\n"
      ],
      "metadata": {
        "id": "MQ0sNuMePBXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code comment is used as inline annotations for your coding"
      ],
      "metadata": {
        "id": "ABD4VhFZbehA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scope of Reproducibility:\n",
        "\n",
        "List hypotheses from the paper you will test and the corresponding experiments you will run.\n",
        "\n",
        "\n",
        "1.   Hypothesis 1:  Large models overfit to small imbalanced datasets.\n",
        "2.   Hypothesis 2:  The proposed model provides better performance compared to other techniques such as vanilla BERT, CNN etc\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uygL9tTPSVHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methodology\n",
        "\n",
        "The code base for the project is uploaded in a public github repo. The repo consists of data and model code used for training the model. Lets start by downloading the code repo"
      ],
      "metadata": {
        "id": "xWAHJ_1CdtaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import  packages you need\n",
        "! git clone https://github.com/aruncs2005/textclassifiers.git\n"
      ],
      "metadata": {
        "id": "yu61Jp1xrnKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Data\n",
        "The data used in the paper is not openly available for further experiments. For this project we will use similar text classification datasets. We are using multiple datasets to test the models general- ization capabilities. Also using a different dataset from the original paper helps us to validate the hypothesis made by authors. Below are the datasets we use for this project\n",
        "1. Medical-Abstracts-TC-Corpus - This repository contains a medical abstracts dataset, de- scribing 5 different classes of patient conditions including Neoplasms,Digestive system diseases, Nervous system diseases,Cardivascular diseases and General pathological conditions. The dataset can be used for text classification. The dataset can be found here - https://github.com/sebischair/Medical-Abstracts-TC-Corpus\n",
        "2. Illness-dataset - the dataset consists of 22,660 documents (tweets) collected in 2018 and 2019. It spans across four domains: Alzheimer’s, Parkinson’s, Cancer, and Diabetes. We will reduce the number of samples to match the requirements depicted in the paper. The dataset can be found here - https://github.com/p-karisani/illness-dataset\n",
        "3.PUBHEALTH - a comprehensive dataset for explainable automated fact-checking of public health claims. Each instance in the PUBHEALTH dataset has an associated veracity label (true, false, unproven, mixture). The dataset can be found here - https://github.com/neemakot/Health-Fact-Checking"
      ],
      "metadata": {
        "id": "2NbPHUTMbkD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the draft we will work with the Medical-Abstract-TC-Corpus. The ideas we use for data curation will be similar for other datasets as well. Lets start with loading and printing the data statistics."
      ],
      "metadata": {
        "id": "Y33qL0Xzis1w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset consists for 5 labels and the dataset is slightly imbalanced. Also we see each label has atleast 1000 samples. For preprocessing we will do the below\n",
        "\n",
        "1.   Reduce the number of samples to approximately around 100\n",
        "2.   Downsample data for some of the labels to make the dataset more imbalanced.\n",
        "3. Filter out large texts to make sure we stick to the hypothesis.\n",
        "\n"
      ],
      "metadata": {
        "id": "gPY8Id6GjAoX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZScZNbROw-N"
      },
      "outputs": [],
      "source": [
        "# dir and function to load raw data\n",
        "import pandas as pd\n",
        "raw_data_dir = 'textclassifiers/data/medabstracts/medical_tc_train.csv'\n",
        "\n",
        "def load_raw_data(raw_data_dir):\n",
        "  # implement this function to load raw data to dataframe/numpy array/tensor\n",
        "  return pd.read_csv(raw_data_dir)\n",
        "\n",
        "raw_data = load_raw_data(raw_data_dir)\n",
        "\n",
        "# calculate statistics\n",
        "def calculate_stats(raw_data):\n",
        "  # implement this function to calculate the statistics\n",
        "  # it is encouraged to print out the results\n",
        "  return raw_data.groupby(\"condition_label\").count()\n",
        "\n",
        "# process raw data\n",
        "def process_data(raw_data):\n",
        "    # implement this function to process the data as you need\n",
        "    sample = 500\n",
        "    col_name = \"condition_label\"\n",
        "\n",
        "    probs = raw_data[col_name].map(raw_data[col_name].value_counts())\n",
        "    sampled_data = raw_data.sample(n=sample, weights=probs)\n",
        "    return sampled_data\n",
        "\n",
        "processed_data = process_data(raw_data)\n",
        "print(\"The original data stats before preprocessing\")\n",
        "print(calculate_stats(raw_data))\n",
        "print(\"The data stats after preprocessing\")\n",
        "print(calculate_stats(processed_data))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is reduce to 500 samples and the labels are also imbalanced especially label 2 and 3."
      ],
      "metadata": {
        "id": "yuJFr5inptgu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the dataset to be used for training"
      ],
      "metadata": {
        "id": "h9aYLkXqr_pB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processed_data.to_csv(\"textclassifiers/data/medabstracts/train.csv\",index=False)"
      ],
      "metadata": {
        "id": "rZGRnWy8sC8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##   Model\n",
        "\n",
        "\n",
        "The project uses BERT and its variations to test different hypothesis. BERT is a Decoder only Transformer Architecture and uses AutoEncoding objective for training. BERT is generally trained in 2 phases\n",
        "1. Unsupervised Pretraining on Large corpus of Data, followed by\n",
        "2. Fine tuning on supervised data like text classification, Question Answering etc.\n",
        "\n",
        "BERT is pretrained on two unsupervised objectives\n",
        "\n",
        "1) Masked LM - Here random input tokens are re- place by a < MASK > token and fed into the network. The goal is to learn from deep bidirectional context and predict the < MASK > tokens in the input sequence. The task uses cross entropy loss to compare predicted tokens with ground truth and update the model parameters\n",
        "\n",
        "2) Next Sentence Prediction (NSP)- In order to make sure the model can be used on downstream tasks such as Ques- tion Answering which spans multiple sentences, BERT is trained to predict whether 2 sentences in a corpus occur one after the other. NSP uses cross entropy as loss function to update the model parameters.\n",
        "\n",
        "BERT was trained on a large corpus of unstructed data spanning to 16 GB in size which included Book Corpus and English wikipedia. BERT was trained on 2 model sizes which includes BERT BASE (110 million Parameters) and BERT LARGE (340M parameters)\n",
        "\n",
        "In this project we use BERT for text classification. The below image shows the high level architecture\n",
        "\n",
        "\n",
        "\n",
        "<img  src='data:image/png;base64,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'>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3muyDPFPbozY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT model structure for text classification.Token and position embeddings are summed up as input embeddings, then fed through N transformer encoder layers, yielding a high-level representations (features) for classification.\n"
      ],
      "metadata": {
        "id": "XUc2G3Roym3z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The image below describes the usage of label embeddings along with BERT. This mainly consists of\n",
        "\n",
        "(a) Incorporating label embeddings to the multi-head self-attention in BERT.\n",
        "\n",
        "(b) Modifying self-attention scores with label embeddings. L indicates row concatenation\n",
        "\n",
        "\n",
        "<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAbQAAABzCAMAAADHcAfMAAABlVBMVEX////y8vLY4vONqducwuXV2+Wzxuf/8ssAAAAvVJb29vb7+/vb5fb39/fo6euWv+TL0N7h4eFERESvt8W1tbV7e3vCy9qenp778tjh6O/08u328ueMmrOCnMy60unc4uyJkqOpqanNzc2Yn6uFhITX19dpaWlfYmbIzdWeqb1SVVlgZm6anqZvhaqputirs78AGT4xPlXFxcWvr69PYYWKptYpNEhaY3KiqreNjY1kbXwgQnwsT44iO2p/godKVm1fc5h3hJvHvZ+yqY5KSkpwi6Q5OTnt4b19m7e6r6HF2/AfHx/c0a+YkHmPstKBoL1gd43Uyak/Tlzu///r3tWKnpjazLqElIG0o5Tu9v+YcV92hpU1M2RZPD3OwLZqdnpgXVSAbF/x3suyssl+bm5CQ3NzRkaapIyhknjQs59SYlNhR1EYE0ZsdF9oWT6jm4JLQDifhXVlSjDXu7j63sBEb4zGxLL//+lQfqYbICgABCjv5dLat6KgjH4qOl2NscJNLiukkJm7m3ZRL193R3Okm7toTm2LWRkeAAARJ0lEQVR4nO2di2PaRp7HRykUn8aSLRihAlvbkhE4lCTtqLu2cndOw8vgh8Axm4bLlk29udtkez1f73zdR5LbTff2/u6dEW80YJBByMTfOEKMRjDSh9+85ycArqvUtT/hVl5LXVEWnYRbTSuxJC06CbeaWoVFJ+BW0yu86AQQwetq0RfgtXwADa5cV9yiL8Fj+QLadsSp/T1GYGSfEba3HNC4iQXDcKJ480wtXImsOrW3zghc3WaERWcJTcO4Ql/rv5zdZ04m30KrN+xtjG61VQCkOGhBuzOs1b2oI4yEbjNizhLa0y2SPBDHIPG1tMPpQIwLcRHE45sz+4aRYt53niVQAszwOUF7RnBB7jwqAgGqh+BpzGj4C9q/bJBN3ag/T3wNfgW+ie+iL5u/5l+gX0/6AeRWSZqrr2YxE3XE0gkzVJPmA63SAAnp2/PIy6dZGR0CA2TX/QUtf5dsfvwNeBn/Gvwr+DeQJNvfrr4C5oTnq2ZKNHVXX82yMz00jZ7ODdpr/afz1Wf5GDQOQfNM9ZmlJV5iXao/1yr138EX+nfxzNm/x76P/kcsLU50Ol8iG9ndV7Ms7ZOpoIXmA001HnBYhygGKrFmEIAmLd38BM3O30gODgQOkiwccpCUFDwncMJkZ+Mj3v0X+xQaU/6Cdk1pSbdnOpEJ/dBy5P9J6OAiFLoguyf5g5NbaLPRkSJPlo8y1H+7oUilDkA7IX8HuXwo9P3pQej04vg4lz/OneZoyIcMjeZssJu9tRom00kQJ8xGGeq723yqVMhkMitmD9rxASGWuwjlQxfH31O7O8nlcxcHxwfHi4Pmi8b1C1Lff3Wv8y7u/v67Ut/dLmDaoyposAeNIDsheWI+F8rnTvKh/GnoOJ8jZpbPnywI2tYDhyL7W3cZ2o84Y267hSaZmZTSNzR1+J+g/kMVNBGsRJqb9dX4jhFDMXUdRxI7FanStjxRuqZGpKZXlCm41ePB+7oi8mAzOKxN9dOPGJqhpYlHSS6tSZne2P3G6V3tshrfebUB/isKftr4743LamJL/1viD5fWm+fgf1qxCtft3h6Rnh60ZLdvY5HQsDHuKIHmYBYMjoI2qzJNomelyM9eK3WCCKt1gunujxsAvwSvKTSLQvv9H+hrF1o2tulQ7CErtff+8R+cuhIal+p0LfI7B1MgO8Azhab4EJqNKkU7L7R0O2gHSECPcmgniIEeq2+Km/UdIRo3JFQPJqJx3IpVMJ35QnBzYmj/dLWlGV1L48U4Sy+YoSI3S2i6ZILWFduNaVBfpVt8CACi45fXh+aiQYvtBNnQQGaqfqe5Q+NVR9fvoGBp/PGZQEtKMmjZWqUBICedR4KAixkEWpP2+l8fGtY0TSeixbxIfnC8MG4wG6U0DhTs9lTKrhVoYzOCfqm6P6BdEWEG0JAGMmLL0todxhs/5AwTzQ5aMinLciqVSqczmQJR2Bbdo82dNDlCjieTpqkoimrIpDKgHCkqUcGkWzmlKi2ZDNkHSCzDQPhopaCUsh5CGzE0A+c+NCMIUECoDS3xTH9VX1VeBzl1dtCmmyRirMgqKIkCufqUTrYQK+R67T6I/uo8sVxNwxgjhAxDbWEluJNHHkLj1zdYqjJDo8PUrgONSmpZGkIRoIkAN0CTox3GiGZPXldE7IwxaZdjrexRRZOeakqeZo/8+lpgcq0NU5vqrkyphdQeObt5ZldEhKOpzvQSmjUFs0Dg58sODSTpuKYNLTzdoKWX0DamgrbmLbRF9IiYBQnIpGFWwtOdd83G9dJA29/by9p/nVfyx+57/OJLhlz2Pepm+ChcMKc9uVDKMPTHLxj6xZ8+d+hPywKNZWlZZi//Pzt/uuS36zp5KRdzcBTSeBinwovxx0eNkk4GbVzNxGNojDItyxxPmzG0kutx5tFSJq6LDqp7rwXFCc0qWgGLEiu2d8n+Wgtg2fqwoIkrpasjTasZQOMc0NZqxWqxWKwVa3TXKpd3i7UnxXKtWiPv5gwN79CtZE9Z1ckWj25cewFNXllRZ3RlPc3F0tZqgVqgaFnFYoDuBMq7gSc1q1orB2rWvKHRyapS4/xQABKdrBpMPAeLhKZlVHdzFMdpLtDKNZItrhXXyuVAtbZG8BFWllUtF6vV4tqcoVUa4A1+d76RfCpWaN8jaIKFlmmp2TObD7SJND9ob/FP56vvXp0laN9jhAbeQrPlW2jNhqjrWAqKQJWkGBARHVC7hWbLt9CYuoVmiwVtKmaeN66dYk6ha3xg0KJTQbM8hZZJOxVOf8YQo1/o88+XFhon3l9jKcAMtTjOQ2jmFf0+V8r1tFK/Qxsxci3Pf+Tax/I9NJZ4bQVNNElkttc1WkZkOER5QLeJnw0Fn99RgnvX/jp/QkuOZyGoK4rgJ2jvg0MBby2AQXwHVIfChcb2qtu7Yxqd6Vf+hKZo4w0JhCdbVT/b6xotB7TLQ4S23il3/uwIz7qFJmugcz2+hMZLJW6sJfnBJUW/3u8MBVweVkTtnciVHeHZVZedvVxvcqovoXG8XlDFcQ6KfAZNGv4mrhHHME4nbA8q3hAFd77YuPaUcCp/QiN1RiUTHid/QfNAqtGbUexTaKRcG+sK7MODBrje1H/fQhsrv5VpHmsqaBDjXokbHzm75BbanDWdpanGm//tvvlmVKyZQmPW/WHpFtrEQujNIxCv7CXevdsj0JpmkPw/q6vRgVizhCZqn7B0ygytB28aNGNrm6E/Xnep7cBEVmTunoH3OCl+B344+wbgp39+vZqI7+OHAymZITRem2r17pzcLM1PWdYM349+sc8YqQvufszQ449+7tQQNFR/CJ9F6/A78IxY2v5l+e22JL3kBs11lpbm34XyM9EoaIx596OgOTUIDSpZ0NwCShT8BcUSyWBFz4JKBNTlQb+Cs4PmZ+8GM9G1oW1dCa2r+l8aY1IyC2i83R7TBaebpXwodHFM9nILdrM0E3kJLT7WvdkMoEFdzmQKmRW5H9pF6PTk4PQiFHqRy4eOqf+eXO4il89f3ChoONY8/JFcbBTH5gaNtMaU8HTdZdeHBtU0lkRRTInDbpZOQvlQPvdtKHQaOskTYgenB8eL8djjUp9skwutAJB4JMK5QZMzpApJV2xPrtK1e/n1DLSHoWFfmZYLnZ7m88fEyKiFhUgOeXx6ekCgHSzGN5ZLvaHjotRJdMKMzi97FJXwCm4vyJ5QLh8T0L3Xgil12tTLVxEx8OamjI34DpojNPI9ujeezXrQeiPXb6ZidnADoAHbc0z7d+1lRWQ+6kHrTaGbrnE9WzdLHmgu7bSPFgyNFG8Jlk6YodKwnyXv0u5Sc7G0BUMjtRHe/j+0C0sc3xfSOSAMRr0B0JKM+a7p9HW7HhcLTVVFSTUErEq8Ql80EsLpKgJHisQpiITooqoKmop5wxmV9z80lTGzfHMz8tWnDH22HnVqixEW3XftkXh6OaCJCLbthhu0KxgWWJY2EJU3bgI0RjYYDO7cY2V6n03qaHd128MLd1oaHjGTzguHZvMUTre7AaeDxliPw3Arc2fB0EbNo4Olq+eq8rp/LU16395ZRmh08vdmjKGHrMBNsd/+eB+XacsMTTQEyZpm1UysHxryMbTOk3uWEBon8venWp824NAMtso0gfkAKOQ1UIXZt7ec0D6eClr/StBO7ZHtUV5097Qw10LsBywtITQRCXTNdbFcDgR2qZ+eANkJVFuLPgN9XgWtAI0yBE31ETSoKsxkLCM0w4a2Ww5UizWrSOA8scpWrVqu1crFqlUuFstWoFa2rGJtrVYcgsZJ3AA0TEjpoDO73mNoSbnAhsbqr7rZ0EgbmUKzqHslAq1YLa/VqmvFWpF6XKLMasTsyrXdAEFarg1D4wahIaAgRTHauZS30DQFsJ3sKDssIXaPyJ11h4JZZ9j6+tYie0Q43YZGjIrwsso0dyxaxXK1aFXJe7KprpG8khyolquWbWkCLwntpbzQgPaIUhuaKGsqQprWHqD1FpqhA8SEZv6MpcdbD/ac+uIeQ49Y5+8utBurlT1OURHRwmncrh5iGdtS/VCmjVKW1XP/8X3W86JWH7HyzK+Y4zWL7caaFpoaLqgdb/Wq7ZdeNSUgSgJoLaPkOotP/A7NWVLduSnQBKfHnl1aeaTlV6DIgAZ50VCgQAX1bvaIJF3TdEXQpaRkSjqH9YVCw3aTLU7zsGWEpjktrUibAKSqSOoilgPbGi8oYgYMV/l1SddlCZuqhjSEMVLxIqFtNexH11zS0nUJobHKNAKq+qhmPaFVyBoDWlJLtR53zeMuNAlonCbpIn00hE4IKtoioRUbFUXC336ypNDatccBPSH1/0e1ovWkultjQOP4kma2Zh0I3KjGNSZF2wKhxb6U3t/dPF9WaG66saCgcQOWZmjDIiUcWhi0xP56Mwqz+hvq5WAJoYkIuukw5gfKtBELTL27rHFaImgdF++kTAtaUzxsxgr29z1q/h2a6WoJodFiSYhGIpHHkSE9jw6+396j20GPZqJfzGmMlqhx3YVGq4G0S2pFFHgo9DzPCSuSAKHQfQ8LGAwvz/bzyHVXI7uxGPJ5N1Yve2yVUfqRzvOG2H1SqGCupIGM+hxoZQxTMwfnjfDKDYCWZfUMf3rvAasbePc+Q49Z53+5SGid2ViCgUmruc+fIAiLsIR6kPgjIaPLQx4PbsJkVfYM408ZZdrqncmnhX+16DKtFRDmoNJnSbAkYjPZfQ8VBRoZfWhcRloqaHfY0JjTwhcLrTPvkddXRKmvzCLQSGOrfUwAZgpyEA9N5PdxmVaPR5+2FiItIbReKcZL4RSWIKl8CPZiwxKtnwjUfxanKyWVTkQWaFuc/tF/gsQLvp2NBTnJ+qmVtiWExvVyPF7QsSmXMoqCyRtQIrV7yVDlcME0dZHy0jSR0zGJxRk8IpmnhKFfs8f4K3GZofXXB4llQSghlMxkSqmjTDiTVhAW2qbHCSqGmikKKpJVQ8USUnTsV0sDFFprWG+poLVpOdxL8zY6jiuJrXZar9aBFA4rkmgoio4UhLCOfFumJdSdaHPd3l0iaILZehVH+QQnZdqwSJlGizVS0BGudgPcx3P5u1oqaO3CTNRVbC9CM1RBV3VONTiNhCBUklQkaKrEGYjHqiYYRi+qQQ7YUf1apvVpiaBRx9N8JzscWoRmv4DS8KpPnhH1JkNjyOeNa07AGQ2Mc4frzB6Z8i7tLqVssfTcVLNOpVheBvdZ5z9cRDcWoSbJY/1O3/BFhddQoj1IOBwOOegM9EAD9YpxfqeJpS0TtKeNJocIjljTeezV0ENOEhF7yD1RHI4oBt+f/XU+6euqM9oP+4b9J8vyqOawfLfjEEB36W7Iver3QSJxAcBr9HvnwbeHg+8r1UYzKKLG8PNQgBTFZ/83ryS2ZHZ9AOHedOmZQ+OZ380UJj8e6YjOATKvjDtjaXRGHYEGmk+cB4ehXR4+w4aKNoefhwLqf5s3NK3P5Znc3Zvc+RZpXE+kyVNkpyJMf0q6S3dsrpV4DOPxE6hXGm9XHQdfbwy+v9yoZNdfPuC2hiPqdytn/z+nFLZktmzAzr56y0lYI39sBUuTxZs4QZiS4jK2/adneqkTfTuXwDgI9c2roy5QKboRk5jeJU1x8QHhGSdIpkZJ2hlJ8pK6IfUXr2VDKwn2I2+ElIsPmDk0amOyDkqkGqLM4aEgyyDKSUsD+4cNfQONfOgKuIU2SpQTMoGpkYq27qa65gb0OMmkkIUZgOmvSPawP+EmSSa5EC/jNP1RIw9dpo2UnQhTs+uQ3ldEbobsmfSQt6uP8lWRvRBHTQzYJia6qRh9EEp29zxvFrHVW+RreuP79waK6+aJ/mAGQOcxgLq7pz8uv/4OHELGnTTFhbMAAAAASUVORK5CYII=\">\n",
        "\n"
      ],
      "metadata": {
        "id": "qoayJze8y8kl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets take a look at the model code specifically for label embeddings. We modify the BERT layers such as embeddings, Attention to accomodate the addition of label embeddings. Below code block depicts the changes in each layer."
      ],
      "metadata": {
        "id": "O7Y2v5dpzYcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding layer\n",
        "#self.label_embeddings = nn.Parameter(torch.zeros(num_labels, config.hidden_size))#nn.Embedding(num_labels, config.hidden_size)\n",
        "\n",
        "#Attention layer forward, combine label embeddings with query\n",
        "\n",
        "# lb_embeddings = label_embeddings[label_ids]\n",
        "# lb_embeddings = lb_embeddings.expand(batch_size, num_labels, hidden_size)\n",
        "# mixed_lb_query = self.query(lb_embeddings)\n",
        "# lb_query = self.transpose_for_scores(mixed_lb_query)"
      ],
      "metadata": {
        "id": "Hs3dGBN3wTQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model training cannot be conducted on a colab environment as it uses a specific versions of python and pytorch. We have build a docker file that can be used to create an environment and test the model training."
      ],
      "metadata": {
        "id": "KCW9g7-s08aX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to build the docker environment , cd into the textclassifiers folder and run docker build .\n",
        "\n",
        "# Note :  We need a machine that has docker setup to run the build. For running the build and training the model we used Amazon SageMaker Notebook instance with G4d 2xlarge instance.\n",
        "\n",
        "# Please follow the below instructions to setup the code\n",
        "\n",
        "# clone the repo\n",
        "! git clone https://github.com/aruncs2005/textclassifiers.git\n",
        "\n",
        "# build the docker image\n",
        "! cd textclassifier && docker build .\n",
        "\n",
        "# install nvidia container toolkit\n",
        "# for AL2\n",
        "! sudo yum install -y nvidia-container-toolkit\n",
        "\n",
        "#for ubuntu\n",
        "! sudo apt-get install -y nvidia-container-toolkit\n",
        "\n",
        "\n",
        "# start the docker image\n",
        "\n",
        "! docker run -it  --runtime=nvidia --gpus all -v <full_path_to>/text_classifiers:/workspace/text_classifiers <container_image_id>\n",
        "\n",
        "# once you are in the container to start the training use\n",
        "\n",
        "! bash run_bert.sh # for bert based training\n",
        "! bash run_label_bert.sh # for label embeddings based training\n"
      ],
      "metadata": {
        "id": "jDnQY5OD1Kot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "\n",
        "We have used accuracy and F1 score to test the model. Below are the results when trained on 2 different settings\n",
        "\n",
        "1. BERT Based Model\n",
        "\n",
        "\n",
        "    acc = 0.8259803921568627\n",
        "    acc_and_f1 = 0.8533263305322129\n",
        "    f1 = 0.880672268907563\n",
        "\n",
        "\n",
        "2. BERT with label embeddings\n",
        "\n",
        "\n",
        "    acc = 0.8480392156862745\n",
        "    acc_and_f1 = 0.8714772349617813\n",
        "    f1 = 0.8949152542372881\n",
        "\n"
      ],
      "metadata": {
        "id": "gX6bCcZNuxmz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model comparison"
      ],
      "metadata": {
        "id": "8EAWAy_LwHlV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the label embeddings model performs slightly better than the plain BERT model. We will continue to test the variations with different dataasets."
      ],
      "metadata": {
        "id": "XdshlOKY3_yV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion\n",
        "\n",
        "The Draft only explores the label embeddings model that the original paper discusses and compares it with BERT based model. For the final submission we are looking to add the below features\n",
        "\n",
        "1.   Implement other models like CNN based classifiers and distillbert to compare performances\n",
        "2.  Test the hypothesis of model distillation lowers overfitting.\n",
        "3. The codebase is currently running with older versions of torch and transformers. We couldn't get the updated version working for the draft. We will update and simplify the codebase.\n",
        "4. Currently the codebase runs with a docker environment due to its dependencies on certain python and torch version. The upgrade will make sure to eliminate the need of docker builds.\n",
        "5. We will use additional datasets mentioned above to run the tests.\n",
        "6. Provide graphical evaluation results.\n",
        "\n"
      ],
      "metadata": {
        "id": "qH75TNU71eRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# no code is required for this section\n",
        "'''\n",
        "if you want to use an image outside this notebook for explanaition,\n",
        "you can read and plot it here like the Scope of Reproducibility\n",
        "'''"
      ],
      "metadata": {
        "id": "E2VDXo5F4Frm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Video Presentation\n",
        "\n",
        "The video explaining the paper and our experiments is uploaded in google drive and can be publicly accessed using the below link\n",
        "\n",
        "https://drive.google.com/file/d/1iWARpnqWEWQ79NUqiNbt9Ms2hndyRUkR/view?usp=sharing"
      ],
      "metadata": {
        "id": "Zx349LICVpN9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "1.   Sun, J, [paper title], [journal title], [year], [volume]:[issue], doi: [doi link to paper]\n",
        "\n"
      ],
      "metadata": {
        "id": "SHMI2chl9omn"
      }
    }
  ]
}