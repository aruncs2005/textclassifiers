{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQ0sNuMePBXx"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "The project work implements the paper Students need more attention which mainly discusses on modifying Bi-directional Encoder Representations from Transformers(BERT) model to classify text data which are imbalanced and small. The general observation from authors is even though deep learning models like CNN, LSTM, BERT perform better than traditional models like SVM, linear regression they tend to overfit for use cases where the dataset is small and imbalanced.\n",
    "\n",
    "**Problem:**\n",
    "\n",
    "Deep learning models, like BERT, are powerful tools but require a vast amount of data for training.\n",
    "In healthcare, datasets are often limited due to privacy concerns and the nature of medical information.\n",
    "Small datasets can lead to overfitting, where the model performs well on the training data but poorly on unseen data.\n",
    "\n",
    "**Proposed Solution:**\n",
    "\n",
    "The paper introduces a novel framework called LESA-BERT (Label Embeddings for Self-Attention BERT). It builds upon BioBERT, a pre-trained model specifically for biomedical text mining. LESA-BERT incorporates two key elements:\n",
    "\n",
    "**1. Label Embeddings for Self-Attention:** This injects label information into the self-attention mechanism of BERT at every layer. Self-attention allows the model to focus on important parts of the text relevant to the task. By including label information, the model can better learn patterns that differentiate between urgency levels in patient messages.\n",
    "\n",
    "**2. Knowledge Distillation:** Large models like LESA-BERT can be prone to overfitting on small datasets. The paper addresses this by employing a technique called knowledge distillation. Here, a smaller \"student\" model is trained to mimic the predictions of the larger \"teacher\" model (LESA-BERT). This compressed student model inherits the knowledge of the teacher but with fewer parameters, reducing overfitting risks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ABD4VhFZbehA"
   },
   "outputs": [],
   "source": [
    "# code comment is used as inline annotations for your coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uygL9tTPSVHB"
   },
   "source": [
    "# Scope of Reproducibility:\n",
    "\n",
    "Below are the hypothesus from the paper we will test.\n",
    "\n",
    "1.   Hypothesis 1:  Large models overfit to small imbalanced datasets.\n",
    "2.   Hypothesis 2:  The proposed model LESA-BERT provides better performance compared to other techniques such as vanilla BERT, CNN etc\n",
    "\n",
    "In order to run the experiments we will train the below models with multiple datasets and different hyperparameter setting\n",
    "\n",
    "1. BERT base\n",
    "2. LESA BERT\n",
    "3. Distil BERT\n",
    "4. CNN\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWAHJ_1CdtaA"
   },
   "source": [
    "# Methodology\n",
    "\n",
    "The code base for the project is uploaded in a public github repo. The repo consists of data and model code used for training the model. Lets start by downloading the code repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yu61Jp1xrnKk"
   },
   "outputs": [],
   "source": [
    "# import  packages you need\n",
    "! git clone https://github.com/aruncs2005/textclassifiers.git\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9QesPSw5lNPD"
   },
   "source": [
    "## Environment Setup\n",
    "\n",
    "We use docker based environment as the code needs specific versions of libraries and CUDA version to run the training job. The training environment is depicted below\n",
    "\n",
    "1. Python Version - 3.6\n",
    "2. Pytorch version - 1.6\n",
    "3. CUDA version - 9.2\n",
    "\n",
    "All the training jobs are run using T4 GPU on AWS. We used Amazon SageMaker notebook instance with G4dn.xlarge as instance type. The code can be reproduced using similar instances or even a CPU based instance which has docker installed.\n",
    "\n",
    "\n",
    "### 1. Prerequisities\n",
    "\n",
    "Install docker - The host machine should have docker setup inorder to build the image and run training jobs. Please use the below link to setup docker based on the OS on your host machine - https://docs.docker.com/engine/install/\n",
    "\n",
    "Note: Amazon SageMaker notebook instances come with docker preinstalled.\n",
    "\n",
    "\n",
    "### 2. Build the docker image\n",
    "\n",
    "Please follow the below instructions to setup the code\n",
    "\n",
    "#### clone the repo\n",
    "``` git clone https://github.com/aruncs2005/textclassifiers.git ```\n",
    "\n",
    "#### build the docker image\n",
    "``` cd textclassifiers && docker build -t image_classifier . ```\n",
    "\n",
    "#### install nvidia container toolkit\n",
    "\n",
    "#### To install on AL2\n",
    "``` sudo yum install -y nvidia-container-toolkit ```\n",
    "\n",
    "#### To install on ubuntu\n",
    "``` sudo apt-get install -y nvidia-container-toolkit ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2NbPHUTMbkD3"
   },
   "source": [
    "##  Data\n",
    "The data used in the paper is not openly available for further experiments. For this project we will use similar text classification datasets. We are using multiple datasets to test the models general- ization capabilities. Also using a different dataset from the original paper helps us to validate the hypothesis made by authors. Below are the datasets we use for this project\n",
    "1. **Medical-Abstracts-TC-Corpus** - This repository contains a medical abstracts dataset, de- scribing 5 different classes of patient conditions including Neoplasms,Digestive system diseases, Nervous system diseases,Cardivascular diseases and General pathological conditions. The dataset can be used for text classification. The dataset can be found here - https://github.com/sebischair/Medical-Abstracts-TC-Corpus\n",
    "2. **Illness-dataset** - the dataset consists of 22,660 documents (tweets) collected in 2018 and 2019. It spans across four domains: Alzheimer’s, Parkinson’s, Cancer, and Diabetes. We will reduce the number of samples to match the requirements depicted in the paper. The dataset can be found here - https://github.com/p-karisani/illness-dataset\n",
    "3.**Patient Reviews** - The dataset contains German doctor reviews from 2021 Containing 439280 rows, each row stands for a review submitted by a patient and includes a numerical rating as well as text comment about their experience with the doctor. The ratings range from 1 (the best) to 6, and the comments are raw texts that have not been preprocessed or cleaned. We have taken a smaller version of the dataset for this project. The dataset can be found here - https://www.kaggle.com/datasets/thedevastator/german-2021-patient-reviews-and-ratings-of-docto\n",
    "4. **MRPC** - Microsoft Research Paraphrase Corpus (MRPC) is a corpus consists of 5,801 sentence pairs collected from newswire articles. Each pair is labelled if it is a paraphrase or not by human annotators. The whole set is divided into a training subset (4,076 sentence pairs of which 2,753 are paraphrases) and a test subset (1,725 pairs of which 1,147 are paraphrases). The datset can be found here - https://www.microsoft.com/en-us/download/details.aspx?id=52398"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y33qL0Xzis1w"
   },
   "source": [
    "Lets take a look at each dataset and prepare the dataset required for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ICUQj3aS9sOF"
   },
   "source": [
    "#### 1. Medical-Abstracts-TC-Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gPY8Id6GjAoX"
   },
   "source": [
    "The dataset consists for 5 labels and the dataset is slightly imbalanced. Also we see each label has atleast 1000 samples. For preprocessing we will do the below\n",
    "\n",
    "1.   Reduce the number of samples to approximately around 1000\n",
    "2.   Downsample data for some of the labels to make the dataset more imbalanced.\n",
    "3. Filter out large texts to make sure we stick to the hypothesis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BZScZNbROw-N"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def main():\n",
    "    data = pd.read_csv(\"data/medabstracts/medical_tc_train.csv\")\n",
    "    data = data.rename({'condition_label': 'label', 'medical_abstract': 'text'}, axis=1)\n",
    "\n",
    "    data[\"text_length\"] = data[\"text\"].str.split(\" \")\n",
    "    data = data[data['text_length'].map(len) < 100]\n",
    "    data = data.drop('text_length', axis=1)\n",
    "    columns_titles = [\"text\",\"label\"]\n",
    "    data=data.reindex(columns=columns_titles)\n",
    "\n",
    "    sample = 700\n",
    "    col_name = \"label\"\n",
    "\n",
    "    probs = data[col_name].map(data[col_name].value_counts())\n",
    "    data = data.sample(n=sample, weights=probs)\n",
    "    print(f\"Number of samples in train dataset {len(data.index)}\")\n",
    "        \n",
    "    data.to_csv(\"data/medabstracts/train.csv\",index=False)\n",
    "\n",
    "    # process test data\n",
    "\n",
    "    data = pd.read_csv(\"data/medabstracts/medical_tc_test.csv\")\n",
    "    data = data.rename({'condition_label': 'label', 'medical_abstract': 'text'}, axis=1)\n",
    "\n",
    "\n",
    "    data[\"text_length\"] = data[\"text\"].str.split(\" \")\n",
    "    data = data[data['text_length'].map(len) < 100]\n",
    "    data = data.drop('text_length', axis=1)\n",
    "    columns_titles = [\"text\",\"label\"]\n",
    "    data=data.reindex(columns=columns_titles)\n",
    "\n",
    "    sample = 200\n",
    "    col_name = \"label\"\n",
    "\n",
    "    probs = data[col_name].map(data[col_name].value_counts())\n",
    "    data = data.sample(n=sample, weights=probs)\n",
    "    print(f\"Number of samples in test dataset {len(data.index)}\")\n",
    "    data.to_csv(\"data/medabstracts/test.csv\",index=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data statistics\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "label_to_description = {1:\"Neoplasms\",2:\"Digestive system diseases\",3:\"Nervous system diseases\",4:\"Cardiovascular diseases\",5:\"General pathological conditions\"}\n",
    "\n",
    "data = pd.read_csv(\"data/medabstracts/train.csv\")\n",
    "data[\"label_names\"] = data[\"label\"].map(label_to_description)\n",
    "print(data.groupby(\"label_names\").count())\n",
    "\n",
    "data.hist(column=\"label\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yuJFr5inptgu"
   },
   "source": [
    "The dataset is reduce to 900 samples and the labels are also imbalanced especially label 2 and 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cAk_JXw91yd"
   },
   "source": [
    "#### 2. Illness Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8DXS1maKbNPa"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def remove_urls(text, replacement_text=\"\"):\n",
    "    # Define a regex pattern to match URLs\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "\n",
    "    # Use the sub() method to replace URLs with the specified replacement text\n",
    "    text_without_urls = url_pattern.sub(replacement_text, text)\n",
    "\n",
    "    return text_without_urls\n",
    "\n",
    "def main():\n",
    "    mapping = {\"alzheimer\":1,\"parkinson\":2,\"cancer\":3,\"diabetes\":4}\n",
    "    file = open(\"textclassifiers/data/illness/data.txt\",encoding=\"utf-8\")\n",
    "\n",
    "    Lines = file.readlines()\n",
    "    dataset = []\n",
    "    for line in Lines:\n",
    "        input = line.split(\"\\t\")\n",
    "        item = {}\n",
    "        item[\"text\"] = remove_urls(input[2]).replace(\"\\\"\",\"\")\n",
    "        item[\"label\"] = mapping[input[0]]\n",
    "        dataset.append(item)\n",
    "\n",
    "    dataDf = pd.DataFrame(dataset)\n",
    "\n",
    "    sample = 1000\n",
    "    col_name = \"label\"\n",
    "\n",
    "    probs = dataDf[col_name].map(dataDf[col_name].value_counts())\n",
    "    dataDf = dataDf.sample(n=sample, weights=probs)\n",
    "\n",
    "    #print(dataDf.head())\n",
    "    traindf, testdf = train_test_split(dataDf, test_size=0.2, random_state=42, shuffle=True)\n",
    "    print(f\"Number of samples in train dataset {len(traindf.index)}\")\n",
    "    print(f\"Number of samples in test dataset {len(testdf.index)}\")\n",
    "\n",
    "    traindf.to_csv(\"data/illness/train.csv\",index=False)\n",
    "    testdf.to_csv(\"data/illness/test.csv\",index=False)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data statistics\n",
    "\n",
    "%matplotlib inline\n",
    "label_to_description = {1:\"alzheimer\",2:\"parkinson\",3:\"cancer\",4:\"diabetes\"}\n",
    "\n",
    "data = pd.read_csv(\"data/illness/train.csv\")\n",
    "data[\"label_names\"] = data[\"label\"].map(label_to_description)\n",
    "print(data.groupby(\"label_names\").count())\n",
    "\n",
    "data.hist(column=\"label\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_4nxux3D95nF"
   },
   "source": [
    "#### 3. Patient Reviews Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datdf = pd.read_csv(\"data/patient_reviews/reviews_small.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the data representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datdf.groupby(\"rating\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Rating 1 has very high number of samples we will sample the data to filter out rating 1 and still keep the dataset imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = datdf[datdf[\"rating\"]==1.0].head(350)\n",
    "data2 = datdf[datdf[\"rating\"]==2.0].head(125)\n",
    "data3 = datdf[datdf[\"rating\"]==3.0].head(25)\n",
    "data4 = datdf[datdf[\"rating\"]==4.0].head(50)\n",
    "data5 = datdf[datdf[\"rating\"]==5.0].head(70)\n",
    "data6 = datdf[datdf[\"rating\"]==6.0].head(60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDF = pd.concat([data1, data2, data3, data4, data5, data6], sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataDF.groupby(\"rating\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save the resulting dataframe for preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDF.to_csv(\"data/patient_reviews/reviews_sampled.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D7Eiay10bGov"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def main():\n",
    "    trainData = pd.read_csv(\"data/patient_reviews/reviews_sampled.csv\")\n",
    "\n",
    "    trainData = trainData[[\"rating\",\"comment\"]]\n",
    "    trainData = trainData.rename({'rating': 'label', 'comment': 'text'}, axis=1)\n",
    "\n",
    "    trainData[\"text_length\"] = trainData[\"text\"].str.split(\" \")\n",
    "    trainData = trainData[trainData['text_length'].map(len) < 100]\n",
    "    trainData = trainData.drop('text_length', axis=1)\n",
    "\n",
    "    columns_titles = [\"text\",\"label\"]\n",
    "    trainData=trainData.reindex(columns=columns_titles)\n",
    "\n",
    "\n",
    "    traindf, testdf = train_test_split(trainData, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "    print(f\"Number of samples in train dataset {len(traindf.index)}\")\n",
    "    print(f\"Number of samples in test dataset {len(testdf.index)}\")\n",
    "\n",
    "    traindf.to_csv(\"data/patient_reviews/train.csv\",index=False)\n",
    "    testdf.to_csv(\"data/patient_reviews/test.csv\",index=False)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data statistics\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "data = pd.read_csv(\"data/patient_reviews/reviews_sampled.csv\")\n",
    "print(data.groupby(\"rating\").count())\n",
    "\n",
    "data.hist(column=\"rating\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2aeCPcT99uS"
   },
   "source": [
    "#### 4. MRPC dataset\n",
    "\n",
    "This dataset is used to reproduce the open sourced observation from the author. This dataset is part of glue tasks and doesn't need any pre processing. We will use the downloaded data at data/MRPC to run the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrpcDF = pd.read_csv(\"data/MRPC/train.tsv\",sep=\"\\t\",on_bad_lines=\"skip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         #1 ID  #2 ID  #1 String  #2 String\n",
      "Quality                                    \n",
      "0         1155   1155       1155       1150\n",
      "1         2394   2394       2394       2379\n"
     ]
    }
   ],
   "source": [
    "print(mrpcDF.groupby(\"Quality\").count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is imbalanced with label 1 having twice the number of samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3muyDPFPbozY"
   },
   "source": [
    "##   Model\n",
    "\n",
    "### BERT\n",
    "\n",
    "The project uses BERT and its variations to test different hypothesis. BERT is a Decoder only Transformer Architecture and uses AutoEncoding objective for training. BERT is generally trained in 2 phases\n",
    "1. Unsupervised Pretraining on Large corpus of Data, followed by\n",
    "2. Fine tuning on supervised data like text classification, Question Answering etc.\n",
    "\n",
    "BERT is pretrained on two unsupervised objectives\n",
    "\n",
    "1) Masked LM - Here random input tokens are re- place by a < MASK > token and fed into the network. The goal is to learn from deep bidirectional context and predict the < MASK > tokens in the input sequence. The task uses cross entropy loss to compare predicted tokens with ground truth and update the model parameters\n",
    "\n",
    "2) Next Sentence Prediction (NSP)- In order to make sure the model can be used on downstream tasks such as Ques- tion Answering which spans multiple sentences, BERT is trained to predict whether 2 sentences in a corpus occur one after the other. NSP uses cross entropy as loss function to update the model parameters.\n",
    "\n",
    "BERT was trained on a large corpus of unstructed data spanning to 16 GB in size which included Book Corpus and English wikipedia. BERT was trained on 2 model sizes which includes BERT BASE (110 million Parameters) and BERT LARGE (340M parameters)\n",
    "\n",
    "In this project we use BERT for text classification. The below image shows the high level architecture\n",
    "\n",
    "\n",
    "\n",
    "<img  src='data:image/png;base64,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'>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XUc2G3Roym3z"
   },
   "source": [
    "BERT model structure for text classification.Token and position embeddings are summed up as input embeddings, then fed through N transformer encoder layers, yielding a high-level representations (features) for classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qoayJze8y8kl"
   },
   "source": [
    "The image below describes the usage of label embeddings along with BERT. This mainly consists of\n",
    "\n",
    "(a) Incorporating label embeddings to the multi-head self-attention in BERT.\n",
    "\n",
    "(b) Modifying self-attention scores with label embeddings. L indicates row concatenation\n",
    "\n",
    "\n",
    "<img src=\"data:image/png;base64,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\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7Y2v5dpzYcn"
   },
   "source": [
    "Lets take a look at the model code specifically for label embeddings. We modify the BERT layers such as embeddings, Attention to accomodate the addition of label embeddings. Below code block depicts the changes in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hs3dGBN3wTQ-"
   },
   "outputs": [],
   "source": [
    "# Embedding layer\n",
    "#self.label_embeddings = nn.Parameter(torch.zeros(num_labels, config.hidden_size))#nn.Embedding(num_labels, config.hidden_size)\n",
    "\n",
    "#Attention layer forward, combine label embeddings with query\n",
    "\n",
    "# lb_embeddings = label_embeddings[label_ids]\n",
    "# lb_embeddings = lb_embeddings.expand(batch_size, num_labels, hidden_size)\n",
    "# mixed_lb_query = self.query(lb_embeddings)\n",
    "# lb_query = self.transpose_for_scores(mixed_lb_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCW9g7-s08aX"
   },
   "source": [
    "The model training cannot be conducted on a colab environment as it uses a specific versions of python and pytorch. We have build a docker file that can be used to create an environment and test the model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train BERT Model\n",
    "\n",
    "To run the model training we have provided a bash script that can be modified with different parameters such as dataset path, learning rate, seq length, batch size, epochs etc\n",
    "\n",
    "The training code can be found under <b>/scripts/run_training_bert.py </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDnQY5OD1Kot"
   },
   "source": [
    "\n",
    "Please follow the below instructions to run the code\n",
    "\n",
    "1. start the docker image\n",
    "\n",
    "```docker run -it  --runtime=nvidia --gpus all -v <full_path_to>/text_classifiers:/workspace/text_classifiers text_classifiers:latest ```\n",
    "\n",
    "once you are in the container to start the training use\n",
    "\n",
    "```cd text_classifiers ```\n",
    "\n",
    "``` bash run_bert.sh ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Statistics\n",
    "\n",
    "Number of training epochs = 10 <br />\n",
    "Average runtime per epoch - 1 m 30 sec <br />\n",
    "Hardware - Nvidia T4 GPU <br />\n",
    "Number of runs = 12 (4 datasets * 3 HPO jobs) <br />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train LabelBERT ( LESABERT )\n",
    "\n",
    "To run the model training we have provided a bash script that can be modified with different parameters such as dataset path, learning rate, seq length, batch size, epochs etc\n",
    "\n",
    "The training code can be found under <b>/scripts/run_training_bert_label.py</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Please follow the below instructions to run the code\n",
    "\n",
    "1. start the docker image\n",
    "\n",
    "```docker run -it  --runtime=nvidia --gpus all -v <full_path_to>/text_classifiers:/workspace/text_classifiers text_classifiers:latest ```\n",
    "\n",
    "once you are in the container to start the training use\n",
    "\n",
    "```cd text_classifiers ```\n",
    "\n",
    "``` bash run_label_bert.sh ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Statistics\n",
    "\n",
    "Number of training epochs = 10 <br />\n",
    "Average runtime per epoch - 1 m 30 sec <br />\n",
    "Hardware - Nvidia T4 GPU <br />\n",
    "Number of runs = 12 (4 datasets * 3 HPO jobs) <br />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQQ37y0NhHZt"
   },
   "source": [
    "### CNN Model\n",
    "\n",
    "Convolutional Neural Networks(CNNs) are structured to automatically learn spatial hierarchies of features from low to high-level patterns. The key components of a CNN include convolution layers, pooling layers, and fully connected layers. Convolution layers apply filters to extract features, pooling layers downsample data, and fully connected layers make final predictions. The below figure shows how CNN can be used for text classification by combining a series of convolutions finally by using a full connected layer for classification.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:4800/format:webp/0*SjaW0zqH8g6VIz7w.png\">\n",
    "\n",
    "\n",
    "\n",
    "The training scripts can be found under <b>scripts/cnn_classification.py</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bh5qGu-1iXCP"
   },
   "source": [
    "\n",
    "Please follow the below instructions to run the code\n",
    "\n",
    "1. start the docker image\n",
    "\n",
    "```docker run -it  --runtime=nvidia --gpus all -v <full_path_to>/text_classifiers:/workspace/text_classifiers text_classifiers:latest ```\n",
    "\n",
    "once you are in the container to start the training use\n",
    "\n",
    "```cd text_classifiers ```\n",
    "\n",
    "``` bash run_cnn.sh ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Statistics\n",
    "\n",
    "Number of training epochs = 10 <br />\n",
    "Average runtime per epoch - 20 sec <br />\n",
    "Hardware - Nvidia T4 GPU <br />\n",
    "Number of runs = 12 (4 datasets * 3 HPO jobs) <br />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Lryb2vUii1M"
   },
   "source": [
    "### DistilBERT\n",
    "\n",
    " DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than google-bert/bert-base-uncased, runs 60% faster while preserving over 95% of BERT’s performances as measured on the GLUE language understanding benchmark.\n",
    "\n",
    " <img src=\"https://www.researchgate.net/profile/Alhassan-Mabrouk/publication/358239462/figure/fig2/AS:1120931644747777@1644262338087/The-DistilBERT-model-architecture-and-components.png\">\n",
    "\n",
    "As shown in teh figure, A smaller DistilBERT model is trained to reproduce the outputs of the pre-trained BERT model on the same data. This is done by defining a loss function that combines three components:\n",
    "**Masked language modeling loss**: To learn the core language modeling task.\n",
    "**Distillation loss**: To match the output probabilities of BERT.\n",
    "**Similarity loss**: To ensure the hidden state embeddings of DistilBERT are similar to BERT's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Du2gYG3rjhQI"
   },
   "source": [
    "\n",
    "Please follow the below instructions to run the code\n",
    "\n",
    "1. start the docker image\n",
    "\n",
    "```docker run -it  --runtime=nvidia --gpus all -v <full_path_to>/text_classifiers:/workspace/text_classifiers text_classifiers:latest ```\n",
    "\n",
    "once you are in the container to start the training use\n",
    "\n",
    "```cd text_classifiers ```\n",
    "\n",
    "``` bash run_distill_bert.sh ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Statistics\n",
    "\n",
    "Number of training epochs = 10 <br />\n",
    "Average runtime per epoch - 1 m  <br />\n",
    "Hardware - Nvidia T4 GPU <br />\n",
    "Number of runs = 12 (4 datasets * 3 HPO jobs) <br />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "### Metrics\n",
    "\n",
    "1. <b>F1 score</b> is a performance metric used to evaluate the accuracy of classification models. It is the harmonic mean of precision and recall, and it provides a balanced measure of a model's performance that takes into account both false positives and false negatives. To compute F1 score we provide both true labels and predicted labels. We have used micro averaging for multiclass datasets\n",
    "\n",
    "2. <b>Accuracy</b> is a metric that measures the proportion of correct predictions made by a model out of the total number of predictions. It is calculated by dividing the sum of true positive and true negative predictions by the total number of predictions, including true positives, true negatives, false positives, and false negatives.\n",
    "\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation code \n",
    "\n",
    "def get_metrics(prediction,label, average=\"micro\"):\n",
    "    batch_size, _ = prediction.shape\n",
    "    predicted_classes = prediction.argmax(dim=-1)\n",
    "    correct_predictions = predicted_classes.eq(label).sum()\n",
    "    accuracy = correct_predictions / batch_size\n",
    "    f1 = f1_score(label, predicted_classes, average=average)\n",
    "    return accuracy,f1\n",
    "\n",
    "\n",
    "def evaluate(data_loader, model, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_losses = []\n",
    "    epoch_accs = []\n",
    "    epoch_f1 = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm.tqdm(data_loader, desc=\"evaluating...\"):\n",
    "            ids = batch[\"ids\"].to(device)\n",
    "            label = batch[\"labels\"].to(device)\n",
    "            prediction = model(ids)\n",
    "            loss = criterion(prediction, label)\n",
    "            accuracy,f1 = get_metrics(prediction.cpu(), label.cpu())\n",
    "            #accuracy = get_accuracy(prediction, label)\n",
    "            #f1 = f1_score(label, prediction, average='micro')\n",
    "            epoch_losses.append(loss.item())\n",
    "            epoch_accs.append(accuracy.item())\n",
    "            epoch_f1.append(f1)\n",
    "\n",
    "    return np.mean(epoch_losses), np.mean(epoch_accs), np.mean(epoch_f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gX6bCcZNuxmz"
   },
   "source": [
    "# Results\n",
    "\n",
    "The below table shows a brief summary of best experiment results for each model on different datasets. We will do a detailed discussion on this table and summarize the findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "| Model        | Dataset       | F1-score       | Accuracy     |\n",
    "| :-------- | :------- | :-------- | :------- |\n",
    "| BERT  |MedAbstracts |0.525  |0.338275    |\n",
    "| LESA-BERT  |MedAbstracts |0.505  |0.3552   |\n",
    "| DistilBert  |MedAbstracts |0.52  |0.3502    |\n",
    "| CNN  |MedAbstracts |0.564 |0.564   |\n",
    "| -------- | ------- | -------- | ------- |\n",
    "| BERT  |Illness | 0.985  |0.2769    |\n",
    "| LESA-BERT  |Illness |0.985  |0.2769   |\n",
    "| DistilBert  |Illness |0.985  | 0.2769    |\n",
    "| CNN  |Illness |0.924 |0.924   |\n",
    "| -------- | ------- | -------- | ------- |\n",
    "| BERT  |patientreviews |0.6153  |0.5439    |\n",
    "| LESA-BERT  |patientreviews |0.5811  |0.533   |\n",
    "| DistilBert  |patientreviews |0.632  |0.552    |\n",
    "| CNN  |patientreviews | 0.469 | 0.469   |\n",
    "| -------- | ------- | -------- | ------- |\n",
    "| BERT  |MRPC |0.8806  |0.8259    |\n",
    "| LESA-BERT  |MRPC |0.8949  |0.8480   |\n",
    "| DistilBert  |MRPC |0.89  |0.8380    |\n",
    "\n",
    "\n",
    "Lets look at the results above and revisit the below 2 hypothesis\n",
    "\n",
    "Hypothesis 1: Large models overfit to small imbalanced datasets. <br>\n",
    "Hypothesis 2: The proposed model LESA-BERT provides better performance compared to other techniques such as vanilla BERT, CNN etc <br>\n",
    "\n",
    "\n",
    "Lets look at the results for each dataset and model\n",
    "\n",
    "1. MedAbstracts - In this case LESA-BERT and DistilBERT performs better compared to BERT model. \n",
    "\n",
    "2. Illness dataset - All models perform similarly on illness dataset. CNN has lesser F1 score.\n",
    "\n",
    "3. Patient Review - DistilBERT and vanilla BERT outperformed the LESA-BERT \n",
    "\n",
    "4. MRPC - LESA-BERT outperforms both BERT and DistilBERT\n",
    "\n",
    "\n",
    "Looking at the results both the hypothesis holds good for some datasets and not always true for other datasets. LESA-BERT performs mostly on par or slightly better than other models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qH75TNU71eRH"
   },
   "source": [
    "# Discussion\n",
    "\n",
    "The project explores the label embeddings model that the original paper discusses and compares it with BERT based model. \n",
    "\n",
    "\n",
    "\n",
    "1. Implications of the experimental results, whether the original paper was reproducible, and if it wasn’t, what factors made it irreproducible. As we were not able to use the dataset form the original paper we couldn't replicate the same results seen by the authors. The LESA-BERT model gave little benefit over other models in text classification when dealing with small imbalanced datasets.\n",
    "\n",
    "2. “What was easy” - The overall setup was revolving around BERT based models and text classification. So it was easy to run experiments and compare. Also it was easy to find medical datasets for test classification and make it small and imbalanced. \n",
    "\n",
    "3. “What was difficult” - The ome original code used legacy version of Transformers and older pytorch version. We had to dockerize to make sure the environment can be reproduced and then update pytorch versions. We tried porting the code to use latest transformer versions but there were some specific code changes related to attention blocks that was not compatible with latest transformers version and would have resulted in major rewrite. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l3GrSUu0a-mN"
   },
   "source": [
    "## Video Presentation\n",
    "\n",
    "The video explaining the paper and our experiments is uploaded in google drive and can be publicly accessed using the below link\n",
    "\n",
    "https://drive.google.com/file/d/1iWARpnqWEWQ79NUqiNbt9Ms2hndyRUkR/view?usp=sharing"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
