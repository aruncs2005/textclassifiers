{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQ0sNuMePBXx"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "The project work implements the paper Students need more attention which mainly discusses on modifying Bi-directional Encoder Representations from Transformers(BERT) model to classify text data which are imbalanced and small. The general observation from authors is even though deep learning models like CNN, LSTM, BERT perform better than traditional models like SVM, linear regression they tend to overfit for use cases where the dataset is small and imbalanced.\n",
        "\n",
        "**Problem:**\n",
        "\n",
        "Deep learning models, like BERT, are powerful tools but require a vast amount of data for training.\n",
        "In healthcare, datasets are often limited due to privacy concerns and the nature of medical information.\n",
        "Small datasets can lead to overfitting, where the model performs well on the training data but poorly on unseen data.\n",
        "\n",
        "**Proposed Solution:**\n",
        "\n",
        "The paper introduces a novel framework called LESA-BERT (Label Embeddings for Self-Attention BERT). It builds upon BioBERT, a pre-trained model specifically for biomedical text mining. LESA-BERT incorporates two key elements:\n",
        "\n",
        "**1. Label Embeddings for Self-Attention:** This injects label information into the self-attention mechanism of BERT at every layer. Self-attention allows the model to focus on important parts of the text relevant to the task. By including label information, the model can better learn patterns that differentiate between urgency levels in patient messages.\n",
        "\n",
        "**2. Knowledge Distillation:** Large models like LESA-BERT can be prone to overfitting on small datasets. The paper addresses this by employing a technique called knowledge distillation. Here, a smaller \"student\" model is trained to mimic the predictions of the larger \"teacher\" model (LESA-BERT). This compressed student model inherits the knowledge of the teacher but with fewer parameters, reducing overfitting risks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABD4VhFZbehA"
      },
      "outputs": [],
      "source": [
        "# code comment is used as inline annotations for your coding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uygL9tTPSVHB"
      },
      "source": [
        "# Scope of Reproducibility:\n",
        "\n",
        "Below are the hypothesus from the paper we will test.\n",
        "\n",
        "1.   Hypothesis 1:  Large models overfit to small imbalanced datasets.\n",
        "2.   Hypothesis 2:  The proposed model LESA-BERT provides better performance compared to other techniques such as vanilla BERT, CNN etc\n",
        "\n",
        "In order to run the experiments we will train the below models with multiple datasets and different hyperparameter setting\n",
        "\n",
        "1. BERT base\n",
        "2. BERT large\n",
        "3. LESA BERT\n",
        "4. Distil BERT\n",
        "5. RoberTA\n",
        "6. CNN\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWAHJ_1CdtaA"
      },
      "source": [
        "# Methodology\n",
        "\n",
        "The code base for the project is uploaded in a public github repo. The repo consists of data and model code used for training the model. Lets start by downloading the code repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yu61Jp1xrnKk"
      },
      "outputs": [],
      "source": [
        "# import  packages you need\n",
        "! git clone https://github.com/aruncs2005/textclassifiers.git\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QesPSw5lNPD"
      },
      "source": [
        "## Environment Setup\n",
        "\n",
        "We use docker based environment as the code needs specific versions of libraries and CUDA version to run the training job. The training environment is depicted below\n",
        "\n",
        "1. Python Version - 3.6\n",
        "2. Pytorch version - 1.6\n",
        "3. CUDA version - 9.2\n",
        "\n",
        "All the training jobs are run using T4 GPU on AWS. We used Amazon SageMaker notebook instance with G4dn.xlarge as instance type. The code can be reproduced using similar instances or even a CPU based instance which has docker installed.\n",
        "\n",
        "\n",
        "### 1. Prerequisities\n",
        "\n",
        "Install docker - The host machine should have docker setup inorder to build the image and run training jobs. Please use the below link to setup docker based on the OS on your host machine - https://docs.docker.com/engine/install/\n",
        "\n",
        "Note: Amazon SageMaker notebook instances come with docker preinstalled.\n",
        "\n",
        "\n",
        "### 2. Build the docker image\n",
        "\n",
        "Please follow the below instructions to setup the code\n",
        "\n",
        "#### clone the repo\n",
        "``` git clone https://github.com/aruncs2005/textclassifiers.git ```\n",
        "\n",
        "#### build the docker image\n",
        "``` cd textclassifiers && docker build -t image_classifier . ```\n",
        "\n",
        "#### install nvidia container toolkit\n",
        "\n",
        "#### To install on AL2\n",
        "``` sudo yum install -y nvidia-container-toolkit ```\n",
        "\n",
        "#### To install on ubuntu\n",
        "``` sudo apt-get install -y nvidia-container-toolkit ```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NbPHUTMbkD3"
      },
      "source": [
        "##  Data\n",
        "The data used in the paper is not openly available for further experiments. For this project we will use similar text classification datasets. We are using multiple datasets to test the models general- ization capabilities. Also using a different dataset from the original paper helps us to validate the hypothesis made by authors. Below are the datasets we use for this project\n",
        "1. **Medical-Abstracts-TC-Corpus** - This repository contains a medical abstracts dataset, de- scribing 5 different classes of patient conditions including Neoplasms,Digestive system diseases, Nervous system diseases,Cardivascular diseases and General pathological conditions. The dataset can be used for text classification. The dataset can be found here - https://github.com/sebischair/Medical-Abstracts-TC-Corpus\n",
        "2. **Illness-dataset** - the dataset consists of 22,660 documents (tweets) collected in 2018 and 2019. It spans across four domains: Alzheimer’s, Parkinson’s, Cancer, and Diabetes. We will reduce the number of samples to match the requirements depicted in the paper. The dataset can be found here - https://github.com/p-karisani/illness-dataset\n",
        "3.**Patient Reviews** - The dataset contains German doctor reviews from 2021 Containing 439280 rows, each row stands for a review submitted by a patient and includes a numerical rating as well as text comment about their experience with the doctor. The ratings range from 1 (the best) to 6, and the comments are raw texts that have not been preprocessed or cleaned. We have taken a smaller version of the dataset for this project. The dataset can be found here - https://www.kaggle.com/datasets/thedevastator/german-2021-patient-reviews-and-ratings-of-docto\n",
        "4. **MRPC** - Microsoft Research Paraphrase Corpus (MRPC) is a corpus consists of 5,801 sentence pairs collected from newswire articles. Each pair is labelled if it is a paraphrase or not by human annotators. The whole set is divided into a training subset (4,076 sentence pairs of which 2,753 are paraphrases) and a test subset (1,725 pairs of which 1,147 are paraphrases). The datset can be found here - https://www.microsoft.com/en-us/download/details.aspx?id=52398"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y33qL0Xzis1w"
      },
      "source": [
        "Lets take a look at each dataset and prepare the dataset required for training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICUQj3aS9sOF"
      },
      "source": [
        "#### 1. Medical-Abstracts-TC-Corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPY8Id6GjAoX"
      },
      "source": [
        "The dataset consists for 5 labels and the dataset is slightly imbalanced. Also we see each label has atleast 1000 samples. For preprocessing we will do the below\n",
        "\n",
        "1.   Reduce the number of samples to approximately around 100\n",
        "2.   Downsample data for some of the labels to make the dataset more imbalanced.\n",
        "3. Filter out large texts to make sure we stick to the hypothesis.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZScZNbROw-N"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def main():\n",
        "    data = pd.read_csv(\"data/medabstracts/medical_tc_train.csv\")\n",
        "    data = data.rename({'condition_label': 'label', 'medical_abstract': 'text'}, axis=1)\n",
        "\n",
        "    data[\"text_length\"] = data[\"text\"].str.split(\" \")\n",
        "    data = data[data['text_length'].map(len) < 100]\n",
        "    data = data.drop('text_length', axis=1)\n",
        "    columns_titles = [\"text\",\"label\"]\n",
        "    data=data.reindex(columns=columns_titles)\n",
        "\n",
        "    sample = 1500\n",
        "    col_name = \"label\"\n",
        "\n",
        "    probs = data[col_name].map(data[col_name].value_counts())\n",
        "    data = data.sample(n=sample, weights=probs)\n",
        "    print(f\"Number of samples in train dataset {len(data.index)}\")\n",
        "    data.to_csv(\"data/medabstracts/train.csv\",index=False)\n",
        "\n",
        "    # process test data\n",
        "\n",
        "    data = pd.read_csv(\"data/medabstracts/medical_tc_test.csv\")\n",
        "    data = data.rename({'condition_label': 'label', 'medical_abstract': 'text'}, axis=1)\n",
        "\n",
        "\n",
        "    data[\"text_length\"] = data[\"text\"].str.split(\" \")\n",
        "    data = data[data['text_length'].map(len) < 100]\n",
        "    data = data.drop('text_length', axis=1)\n",
        "    columns_titles = [\"text\",\"label\"]\n",
        "    data=data.reindex(columns=columns_titles)\n",
        "\n",
        "    sample = 300\n",
        "    col_name = \"label\"\n",
        "\n",
        "    probs = data[col_name].map(data[col_name].value_counts())\n",
        "    data = data.sample(n=sample, weights=probs)\n",
        "    print(f\"Number of samples in test dataset {len(data.index)}\")\n",
        "    data.to_csv(\"data/medabstracts/test.csv\",index=False)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wf4OIDOuAQKK"
      },
      "outputs": [],
      "source": [
        "# Below code does preprocessing and store the train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuJFr5inptgu"
      },
      "source": [
        "[link text](https://)The dataset is reduce to 1100 samples and the labels are also imbalanced especially label 2 and 3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9aYLkXqr_pB"
      },
      "source": [
        "Save the dataset to be used for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZGRnWy8sC8o"
      },
      "outputs": [],
      "source": [
        "processed_data.to_csv(\"data/medabstracts/train.csv\",index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cAk_JXw91yd"
      },
      "source": [
        "#### 2. Illness Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DXS1maKbNPa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def remove_urls(text, replacement_text=\"\"):\n",
        "    # Define a regex pattern to match URLs\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "\n",
        "    # Use the sub() method to replace URLs with the specified replacement text\n",
        "    text_without_urls = url_pattern.sub(replacement_text, text)\n",
        "\n",
        "    return text_without_urls\n",
        "\n",
        "def main():\n",
        "    mapping = {\"alzheimer\":1,\"parkinson\":2,\"cancer\":3,\"diabetes\":4}\n",
        "    file = open(\"textclassifiers/data/illness/data.txt\",encoding=\"utf-8\")\n",
        "\n",
        "    Lines = file.readlines()\n",
        "    dataset = []\n",
        "    for line in Lines:\n",
        "        input = line.split(\"\\t\")\n",
        "        item = {}\n",
        "        item[\"text\"] = remove_urls(input[2]).replace(\"\\\"\",\"\")\n",
        "        item[\"label\"] = mapping[input[0]]\n",
        "        dataset.append(item)\n",
        "\n",
        "    dataDf = pd.DataFrame(dataset)\n",
        "\n",
        "    sample = 1000\n",
        "    col_name = \"label\"\n",
        "\n",
        "    probs = dataDf[col_name].map(dataDf[col_name].value_counts())\n",
        "    dataDf = dataDf.sample(n=sample, weights=probs)\n",
        "\n",
        "    #print(dataDf.head())\n",
        "    traindf, testdf = train_test_split(dataDf, test_size=0.2, random_state=42, shuffle=True)\n",
        "    print(f\"Number of samples in train dataset {len(traindf.index)}\")\n",
        "    print(f\"Number of samples in test dataset {len(testdf.index)}\")\n",
        "\n",
        "    traindf.to_csv(\"data/illness/train.csv\",index=False)\n",
        "    testdf.to_csv(\"data/illness/test.csv\",index=False)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4nxux3D95nF"
      },
      "source": [
        "#### 3. Patient Reviews Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7Eiay10bGov"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def main():\n",
        "    trainData = pd.read_csv(\"data/patient_reviews/reviews_small.csv\")\n",
        "\n",
        "    trainData = trainData[[\"rating\",\"comment\"]]\n",
        "    print(len(trainData.index))\n",
        "    trainData = trainData.rename({'rating': 'label', 'comment': 'text'}, axis=1)\n",
        "\n",
        "    trainData[\"text_length\"] = trainData[\"text\"].str.split(\" \")\n",
        "    trainData = trainData[trainData['text_length'].map(len) < 100]\n",
        "    trainData = trainData.drop('text_length', axis=1)\n",
        "\n",
        "    columns_titles = [\"text\",\"label\"]\n",
        "    trainData=trainData.reindex(columns=columns_titles)\n",
        "\n",
        "\n",
        "    sample = 1500\n",
        "    col_name = \"label\"\n",
        "\n",
        "    probs = trainData[col_name].map(trainData[col_name].value_counts())\n",
        "    trainData = trainData.sample(n=sample, weights=probs)\n",
        "\n",
        "    print(len(trainData.index))\n",
        "\n",
        "    traindf, testdf = train_test_split(trainData, test_size=0.2, random_state=42, shuffle=True)\n",
        "\n",
        "    print(f\"Number of samples in train dataset {len(traindf.index)}\")\n",
        "    print(f\"Number of samples in test dataset {len(testdf.index)}\")\n",
        "\n",
        "    traindf.to_csv(\"data/patient_reviews/train.csv\",index=False)\n",
        "    testdf.to_csv(\"data/patient_reviews/test.csv\",index=False)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2aeCPcT99uS"
      },
      "source": [
        "#### 4. MRPC dataset\n",
        "\n",
        "This dataset is used to reproduce the open sourced observation from the author. This dataset is part of glue tasks and doesn't need any pre processing. We will use the downloaded data at data/MRPC to run the training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3muyDPFPbozY"
      },
      "source": [
        "##   Model\n",
        "\n",
        "### BERT\n",
        "\n",
        "The project uses BERT and its variations to test different hypothesis. BERT is a Decoder only Transformer Architecture and uses AutoEncoding objective for training. BERT is generally trained in 2 phases\n",
        "1. Unsupervised Pretraining on Large corpus of Data, followed by\n",
        "2. Fine tuning on supervised data like text classification, Question Answering etc.\n",
        "\n",
        "BERT is pretrained on two unsupervised objectives\n",
        "\n",
        "1) Masked LM - Here random input tokens are re- place by a < MASK > token and fed into the network. The goal is to learn from deep bidirectional context and predict the < MASK > tokens in the input sequence. The task uses cross entropy loss to compare predicted tokens with ground truth and update the model parameters\n",
        "\n",
        "2) Next Sentence Prediction (NSP)- In order to make sure the model can be used on downstream tasks such as Ques- tion Answering which spans multiple sentences, BERT is trained to predict whether 2 sentences in a corpus occur one after the other. NSP uses cross entropy as loss function to update the model parameters.\n",
        "\n",
        "BERT was trained on a large corpus of unstructed data spanning to 16 GB in size which included Book Corpus and English wikipedia. BERT was trained on 2 model sizes which includes BERT BASE (110 million Parameters) and BERT LARGE (340M parameters)\n",
        "\n",
        "In this project we use BERT for text classification. The below image shows the high level architecture\n",
        "\n",
        "\n",
        "\n",
        "<img  src='data:image/png;base64,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'>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUc2G3Roym3z"
      },
      "source": [
        "BERT model structure for text classification.Token and position embeddings are summed up as input embeddings, then fed through N transformer encoder layers, yielding a high-level representations (features) for classification.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoayJze8y8kl"
      },
      "source": [
        "The image below describes the usage of label embeddings along with BERT. This mainly consists of\n",
        "\n",
        "(a) Incorporating label embeddings to the multi-head self-attention in BERT.\n",
        "\n",
        "(b) Modifying self-attention scores with label embeddings. L indicates row concatenation\n",
        "\n",
        "\n",
        "<img src=\"data:image/png;base64,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\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7Y2v5dpzYcn"
      },
      "source": [
        "Lets take a look at the model code specifically for label embeddings. We modify the BERT layers such as embeddings, Attention to accomodate the addition of label embeddings. Below code block depicts the changes in each layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hs3dGBN3wTQ-"
      },
      "outputs": [],
      "source": [
        "# Embedding layer\n",
        "#self.label_embeddings = nn.Parameter(torch.zeros(num_labels, config.hidden_size))#nn.Embedding(num_labels, config.hidden_size)\n",
        "\n",
        "#Attention layer forward, combine label embeddings with query\n",
        "\n",
        "# lb_embeddings = label_embeddings[label_ids]\n",
        "# lb_embeddings = lb_embeddings.expand(batch_size, num_labels, hidden_size)\n",
        "# mixed_lb_query = self.query(lb_embeddings)\n",
        "# lb_query = self.transpose_for_scores(mixed_lb_query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCW9g7-s08aX"
      },
      "source": [
        "The model training cannot be conducted on a colab environment as it uses a specific versions of python and pytorch. We have build a docker file that can be used to create an environment and test the model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDnQY5OD1Kot"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Please follow the below instructions to setup the code\n",
        "\n",
        "# start the docker image\n",
        "\n",
        "! docker run -it  --runtime=nvidia --gpus all -v <full_path_to>/text_classifiers:/workspace/text_classifiers text_classifiers:latest\n",
        "\n",
        "# once you are in the container to start the training use\n",
        "\n",
        "! bash run_bert.sh # for bert based training\n",
        "! bash run_label_bert.sh # for label embeddings based training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQQ37y0NhHZt"
      },
      "source": [
        "### CNN Model\n",
        "\n",
        "Convolutional Neural Networks(CNNs) are structured to automatically learn spatial hierarchies of features from low to high-level patterns. The key components of a CNN include convolution layers, pooling layers, and fully connected layers. Convolution layers apply filters to extract features, pooling layers downsample data, and fully connected layers make final predictions. The below figure shows how CNN can be used for text classification by combining a series of convolutions finally by using a full connected layer for classification.\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:4800/format:webp/0*SjaW0zqH8g6VIz7w.png\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bh5qGu-1iXCP"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Please follow the below instructions to setup the code\n",
        "\n",
        "# start the docker image\n",
        "\n",
        "! docker run -it  --runtime=nvidia --gpus all -v <full_path_to>/text_classifiers:/workspace/text_classifiers text_classifiers:latest\n",
        "\n",
        "# once you are in the container to start the training use\n",
        "\n",
        "! bash run_cnn.sh # for bert based training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Lryb2vUii1M"
      },
      "source": [
        "### DistilBERT\n",
        "\n",
        " DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than google-bert/bert-base-uncased, runs 60% faster while preserving over 95% of BERT’s performances as measured on the GLUE language understanding benchmark.\n",
        "\n",
        " <img src=\"https://www.researchgate.net/profile/Alhassan-Mabrouk/publication/358239462/figure/fig2/AS:1120931644747777@1644262338087/The-DistilBERT-model-architecture-and-components.png\">\n",
        "\n",
        "As shown in teh figure, A smaller DistilBERT model is trained to reproduce the outputs of the pre-trained BERT model on the same data. This is done by defining a loss function that combines three components:\n",
        "**Masked language modeling loss**: To learn the core language modeling task.\n",
        "**Distillation loss**: To match the output probabilities of BERT.\n",
        "**Similarity loss**: To ensure the hidden state embeddings of DistilBERT are similar to BERT's."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Du2gYG3rjhQI"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Please follow the below instructions to setup the code\n",
        "\n",
        "# start the docker image\n",
        "\n",
        "! docker run -it  --runtime=nvidia --gpus all -v <full_path_to>/text_classifiers:/workspace/text_classifiers text_classifiers:latest\n",
        "\n",
        "# once you are in the container to start the training use\n",
        "\n",
        "! bash run_distil_bert.sh # for bert based training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gX6bCcZNuxmz"
      },
      "source": [
        "# Results\n",
        "\n",
        "We have used accuracy and F1 score to test the model. Below are the results when trained on 2 different settings\n",
        "\n",
        "1. BERT Based Model\n",
        "\n",
        "\n",
        "    acc = 0.8259803921568627\n",
        "    acc_and_f1 = 0.8533263305322129\n",
        "    f1 = 0.880672268907563\n",
        "\n",
        "\n",
        "2. BERT with label embeddings\n",
        "\n",
        "\n",
        "    acc = 0.8480392156862745\n",
        "    acc_and_f1 = 0.8714772349617813\n",
        "    f1 = 0.8949152542372881\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EAWAy_LwHlV"
      },
      "source": [
        "## Model comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdshlOKY3_yV"
      },
      "source": [
        "We can see that the label embeddings model performs slightly better than the plain BERT model. We will continue to test the variations with different dataasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qH75TNU71eRH"
      },
      "source": [
        "# Discussion\n",
        "\n",
        "The Draft only explores the label embeddings model that the original paper discusses and compares it with BERT based model. For the final submission we are looking to add the below features\n",
        "\n",
        "1.   Implement other models like CNN based classifiers and distillbert to compare performances\n",
        "2.  Test the hypothesis of model distillation lowers overfitting.\n",
        "3. The codebase is currently running with older versions of torch and transformers. We couldn't get the updated version working for the draft. We will update and simplify the codebase.\n",
        "4. Currently the codebase runs with a docker environment due to its dependencies on certain python and torch version. The upgrade will make sure to eliminate the need of docker builds.\n",
        "5. We will use additional datasets mentioned above to run the tests.\n",
        "6. Provide graphical evaluation results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2VDXo5F4Frm"
      },
      "outputs": [],
      "source": [
        "# no code is required for this section\n",
        "'''\n",
        "if you want to use an image outside this notebook for explanaition,\n",
        "you can read and plot it here like the Scope of Reproducibility\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3GrSUu0a-mN"
      },
      "source": [
        "## Video Presentation\n",
        "\n",
        "The video explaining the paper and our experiments is uploaded in google drive and can be publicly accessed using the below link\n",
        "\n",
        "https://drive.google.com/file/d/1iWARpnqWEWQ79NUqiNbt9Ms2hndyRUkR/view?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHMI2chl9omn"
      },
      "source": [
        "# References\n",
        "\n",
        "1.   Sun, J, [paper title], [journal title], [year], [volume]:[issue], doi: [doi link to paper]\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
